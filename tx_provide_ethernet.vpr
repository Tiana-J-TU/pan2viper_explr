// Start [manual] translation of `tx_provide()` from `ethernet.pan`

domain myBV interpretation (SMTLIB: "(_ BitVec 64)", Boogie: "bv64") {

   function toBV64(i: Int): myBV interpretation "(_ int2bv 64)"
   function toInt(bv: myBV): Int interpretation "(_ bv2int 64)"

   function xor(b1: myBV, b2: myBV): myBV interpretation "bvxor"
   function and(b1: myBV, b2: myBV): myBV interpretation "bvand"
   function or(b1: myBV, b2: myBV): myBV interpretation "bvor"
   function add(b1: myBV, b2: myBV): myBV interpretation "bvadd"
   function sub(b1: myBV, b2: myBV): myBV interpretation "bvsub"
   function mul(b1: myBV, b2:myBV): myBV interpretation "bvmul"
   function shl(b1: myBV, b2: myBV): myBV interpretation "bvshl" // Pancake `<<`
   function lshr(b1: myBV, b2: myBV): myBV interpretation "bvlshr" // Pancake `>>>`
   function ashr(b1: myBV, b2: myBV): myBV interpretation "bvashr"// Pancake `>>`
   function not(b1: myBV): myBV interpretation "bvnot"

   function toBool(bv: myBV): Bool 

   axiom ax_toBool {
      forall b:myBV :: { toBool(b) }
         (toInt(b) == 0 ==> toBool(b) == true) &&
         (toInt(b) != 0 ==> toBool(b) == false)
   }
}

domain Shape {
   function shape_One(): Shape
   function cons_shape(ls: Seq[Int]): Shape // TODO: refine this

}

domain Record { // Can I parameterise on Shapes?

   // Hopefully the only use case here
   function get_word_field(r: Record,i: Int): myBV 
}
// for now, just set a base address value via a define
define _base toBV64(0) // base addr is at 0
// #define WORD_SIZE 8
define WORD_SIZE toBV64(8)
// #define MAX_COUNT 256
define MAX_COUNT toBV64(256)
// #define NET_BUFF_DESC_SIZE  (2 * WORD_SIZE)
define NET_BUFF_DESC_SIZE mul(toBV64(2), WORD_SIZE)
// #define HW_RING_SIZE (3 * WORD_SIZE + MAX_COUNT * NET_BUFF_DESC_SIZE)
define HW_RING_SIZE add(mul(toBV64(3), WORD_SIZE), mul(MAX_COUNT, NET_BUFF_DESC_SIZE))
// #define TX_COUNT 256
define TX_COUNT toBV64(256)
// #define ETH_FUNC_BASE
define ETH_FUNC_BASE add(_base,toBV64(8360))
// #define TXD_READY (1 << 15)
define TXD_READY shl(toBV64(1),toBV64(15))
// #define TXD_ADDCRC (1 << 10)
define TXD_ADDCRC shl(toBV64(1),toBV64(10))
// #define TXD_LAST (1 << 11)
define TXD_LAST shl(toBV64(1),toBV64(11))
// #define MASK_16 65535
define MASK_16 toBV64(65535)
// #define WRAP (1 << 13)
define WRAP shl(toBV64(1),toBV64(13))
// #define DESCRIPTOR_SIZE WORD_SIZE
define DESCRIPTOR_SIZE WORD_SIZE
// #define TDAR_OFFSET 20
define TDAR_OFFSET toBV64(20)
// #define TDAR_TDAR (1 << 24)
define TDAR_TDAR shl(toBV64(1),toBV64(24))

// #define THREAD_MEMORY_RELEASE()         \
//     @THREAD_MEMORY_RELEASE(0,0,0,0);    \
method EXT_THREAD_MEMORY_RELEASE(a1: myBV, a2: myBV, l1: myBV, l2: myBV)
   requires true
   ensures true
{}

// Double check that dest & src arguments appear in this order 
method shared_ldw(d: myBV, s: myBV) 
   requires true
   ensures true
{}

method shared_stw(a: myBV, v: myBV) 
   requires true
   ensures true
{}

method shared_st8(a: myBV, v: myBV) 
   requires true
   ensures true
{}

// Placeholder addresses - probably inaccurate - I don't know where 
//   exactly all these addresses are yet
// The idea is to compute these fixed addresses in advance (so long as there
// aren't too many of them)
define HW_RING_ST toBV64(0)  // First usable cell address
define HW_RING_END toBV64(8) // First *un*usable (out of bounds) cell address (could be changed to last usable address)
define HW_RING_END_INDEX 8   // The int which corresponds to the BV value above
define HW_RING_LENGTH_INT 8
define HW_RING_0 toBV64(0)
define HW_RING_1 toBV64(1)
define HW_RING_2 toBV64(2*8)
define HW_RING_3 toBV64(3*8)
define HW_RING_4 toBV64(4*8)
define HW_RING_5 toBV64(5*8)
define HW_RING_6 toBV64(6*8)
define HW_RING_7 toBV64(7*8) 
define HW_WRAP toBV64(8*8)
define HW_STAT toBV64(9*8)

field HWS_n_active_bufs: Int
field HWS_head: Int //  between 0-7 [inclusive]
field HWS_tail: Int // between 0-7 [inclusive]
field HWS_proc: Bool
field HWS_err: Bool

define HWS_acc(st) acc(st.HWS_n_active_bufs) && acc(st.HWS_head) && acc(st.HWS_tail) && acc(st.HWS_proc) && acc(st.HWS_err)

method init_HWS() returns (HWS: Ref) 
   requires true
   ensures HWS_acc(HWS)
{
   HWS := new(HWS_n_active_bufs, HWS_head, HWS_tail, HWS_proc, HWS_err)
   HWS.HWS_n_active_bufs := 0 
   HWS.HWS_head := 0
   HWS.HWS_tail := 0
   HWS.HWS_proc := false
   HWS.HWS_err  := false
}

// What is the minimal yet sound device model?
// shared stores write buffers / buffer headers
method shared_st32(a: myBV, v:myBV, HWS: Ref)
   requires HWS_acc(HWS)
   ensures HWS_acc(HWS)
{  
   // Simplified model: does not check that header set correctly
   // Translate concrete address & data into abstract transition
   if (a == HW_RING_0) { transition_write_buffer(HWS,0) }
   if (a == HW_RING_1) { transition_write_buffer(HWS,1) }
   if (a == HW_RING_2) { transition_write_buffer(HWS,2) }
   if (a == HW_RING_3) { transition_write_buffer(HWS,3) }
   if (a == HW_RING_4) { transition_write_buffer(HWS,4) }
   if (a == HW_RING_5) { transition_write_buffer(HWS,5) }
   if (a == HW_RING_6) { transition_write_buffer(HWS,6) }
   if (a == HW_RING_7) { transition_write_buffer(HWS,7) } 
   if (a == HW_STAT) { transition_trigger_proc(HWS) }
   if (a == HW_WRAP) {}
   // best way to assert that all of these addresses are distinct?
   // possibly writing the above in an if-else format bypasses this need

   // we have a finite (and low) number of buffer slots
   // is ==? easy to determine with BVs? If so, potientially more performant
   // to just hardcode address (word) -> index (int) conversion 
}

// Probably good to have this at some point
function check_HWS_integrity(HWS: Ref): Bool 

method transition_ND_processing(HWS: Ref)
   requires HWS_acc(HWS)
   ensures HWS_acc(HWS)
{
   if (HWS.HWS_proc) {
      if (HWS.HWS_n_active_bufs > 0) {
         var n: Int := ND_select(HWS.HWS_n_active_bufs)
         HWS.HWS_n_active_bufs := HWS.HWS_n_active_bufs - n
         HWS.HWS_head := HWS.HWS_head + n
         // Then check integrity of state - not included yet
         // If head now points to an empty buffer, stop processing until driver says to
         if (HWS.HWS_head == HWS.HWS_tail) { HWS.HWS_proc := false }
      }
      // else: nothing to process anyway - this case probably shouldn't occur in practice
   } 
   // else: not in processing state - device isn't informed of anything 
   //   to process at the current time
}

// Head/tail check very similar in style to the method below, perhaps I should
//   separate some of this out into a function
method check_buffer_empty(HWS: Ref, l: Int) returns (buffer_empty: Bool) 
   requires HWS_acc(HWS)
   requires 0 <= l && l < HW_RING_END_INDEX
   ensures HWS_acc(HWS)
{
    if (HWS.HWS_head < HWS.HWS_tail) {
      if (l < HWS.HWS_head || HWS.HWS_tail <= l) {
         buffer_empty := true
      }
      else { buffer_empty := false }
   } // Check if there is room to write buffer at the current location
   elseif (HWS.HWS_head == HWS.HWS_tail) {
      if (HWS.HWS_n_active_bufs == 0) { buffer_empty := true}
      else { buffer_empty := false}
   }
   elseif (HWS.HWS_tail < HWS.HWS_head) {
      if (HWS.HWS_tail <= l && l < HWS.HWS_head) { buffer_empty := true }
      else { buffer_empty := false }
   }
}

define BUFFER_EMPTY toBV64(127) // Just a guess at the value

// shared loads check headers
method shared_ld32(d: myBV, s: myBV, HWS: Ref) 
   requires HWS_acc(HWS)
   ensures HWS_acc(HWS)
{
   var empty_stat: Bool := false
   if (s == HW_RING_0) { empty_stat := check_buffer_empty(HWS, 0) }
   if (s == HW_RING_1) { empty_stat := check_buffer_empty(HWS, 1) }
   if (s == HW_RING_2) { empty_stat := check_buffer_empty(HWS, 2) }
   if (s == HW_RING_3) { empty_stat := check_buffer_empty(HWS, 3) }
   if (s == HW_RING_4) { empty_stat := check_buffer_empty(HWS, 4) }
   if (s == HW_RING_5) { empty_stat := check_buffer_empty(HWS, 5) }
   if (s == HW_RING_6) { empty_stat := check_buffer_empty(HWS, 6) }
   if (s == HW_RING_7) { empty_stat := check_buffer_empty(HWS, 7) } 

   // if (empty_stat) { d := BUFFER_EMPTY } // May have to change the way that this works to update value of d 
   // (which is really a heap value w/ global visibility rather than an isolated BV)
}



method transition_write_buffer(HWS: Ref, l: Int) 
   requires HWS_acc(HWS)
   requires 0 <= l && l < HW_RING_END_INDEX
   ensures HWS_acc(HWS)
{  
   // First update current view of device state to account for any
   //   processesing which has occurred before our write attempt
   if (HWS.HWS_proc) { 
      transition_ND_processing(HWS)
   }

   // Transition immediately to an error state if overwrite occurs, otherwise
   //   update the current state to reflect the newly added active buffer
   if (HWS.HWS_head < HWS.HWS_tail) {
      if (l < HWS.HWS_head || HWS.HWS_tail <= l) {
         HWS.HWS_tail := (HWS.HWS_tail + 1) % HW_RING_LENGTH_INT
         HWS.HWS_n_active_bufs := HWS.HWS_n_active_bufs + 1
      }
      else { HWS.HWS_err := true }
   } // Check if there is room to write buffer at the current location
   elseif (HWS.HWS_head == HWS.HWS_tail) {
      if (HWS.HWS_n_active_bufs == 0) {
         HWS.HWS_tail := (HWS.HWS_tail + 1) % HW_RING_LENGTH_INT
         HWS.HWS_n_active_bufs := HWS.HWS_n_active_bufs + 1
      }
      else { HWS.HWS_err := true }
   }
   elseif (HWS.HWS_tail < HWS.HWS_head) {
      if (HWS.HWS_tail <= l && l < HWS.HWS_head) {
         HWS.HWS_tail := (HWS.HWS_tail + 1) % HW_RING_LENGTH_INT
         HWS.HWS_n_active_bufs := HWS.HWS_n_active_bufs + 1
      }
      else { HWS.HWS_err := true }
   }
}

method transition_trigger_proc(HWS: Ref)
   requires acc(HWS.HWS_proc)
   ensures acc(HWS.HWS_proc)
{
   HWS.HWS_proc := true
}

method net_dequeue_active(q: myBV, a: myBV) returns (r: Bool)
   requires true
   ensures true
{}

method lds(s: Shape, a: myBV) returns (v: Record) 
   requires true
   ensures true
{}

define bv_zero toBV64(0)
// Input param for device state modelling only
method tx_provide(HW_state: Ref) returns (r: Int) 
   requires HWS_acc(HW_state)
   ensures HWS_acc(HW_state)
{
   
  /* 
    var reprocess = true;
    get_hw_ring_tx(hw_ring_tx) // macro
    get_tx_queue(tx_queue)     // macro
    get_eth_regs(eth)          // macro
  */

  var reprocess : Bool := true
  // Expanded macro for `get_hw_ring_tx(hw_ring_tx)`
  var hw_ring_tx: myBV := add(add(_base,mul(toBV64(9),WORD_SIZE)),HW_RING_SIZE)  
  // Expanded macro for `get_tx_queue(tx_queue)`
  var tx_queue: myBV := add(_base,mul(toBV64(6), WORD_SIZE))
  // Expanded macro for `get_eth_regs(eth)`
  var addrs:myBV := _base
  var eth_R: Record := lds(shape_One(), addrs) 
  // Loading the Shape `1` has to be done in two steps (because of how I've encoded `lds` & Shapes)
  var eth: myBV := get_word_field(eth_R,0)

  while (reprocess) invariant HWS_acc(HW_state) {
    while (true) invariant HWS_acc(HW_state) {
      /*
        hw_ring_full(full, hw_ring_tx, TX_COUNT)
        net_queue_empty_active(empty, tx_queue)
        if (full || empty) {
           break;
        }
      */
      
      // Expanded macro for `hw_ring_full(full, hw_ring_tx, TX_COUNT)
        // Expanded macro for `hw_ring_get_tail(tail, hw_ring)` 
      var tail: myBV := toBV64(0)
      shared_ldw(tail, hw_ring_tx)

        // Expanded macro for `hw_ring_get_head(head, hw_ring)`
      var head: myBV := toBV64(0)
      shared_ldw(head, add(hw_ring_tx, WORD_SIZE))

      var value : myBV := sub(tail, add(head, toBV64(1)))
        // Expanded macro for `pnk_modulo(mod, value, ring_size)`
      var pnk_modulo_mask: myBV := sub(TX_COUNT, toBV64(1))
      var mod: myBV := and(value, pnk_modulo_mask)

      var full: Bool := (toInt(mod) == 0)

      // Expanded macro for `net_queue_empty_active(empty, tx_queue)`
      //  Expanded macro for `get_active_tail(tail, tx_queue)`                 
         // Expanded macro for `get_active_queue(tail, tx_queue)`
      tail := toBV64(0)
      shared_ldw(tail, add(tx_queue, WORD_SIZE))

         // Expanded macro for `get_tail(tail, tx_queue)`
      tail := toBV64(0)
      shared_ldw(tail, tx_queue)

      //  Expanded macro for `get_active_head(head, tx_queue)`                 
         // Expanded macro for `get_active_queue(head, tx_queue)`
      tail := toBV64(0)
      shared_ldw(tail, add(tx_queue, WORD_SIZE))

         // `get_head(head, tx_queue)`
      head := toBV64(0)
      shared_ldw(head, add(tx_queue, WORD_SIZE))

      var empty: Bool := toInt(sub(tail,head)) == 0                     
   
      if (full || empty) {
         // break // Well this doesn't work - deal with it later
         // could always use a goto, not sure if this is troublesome
      }

      // --- END OF BLOCK 1 ------------------------------------------------------- //

      var buffer_addr: myBV := add(ETH_FUNC_BASE, toBV64(512))
      var err: Bool := net_dequeue_active(tx_queue, buffer_addr) // Not sure about typing here
      inhale !err // temporary assumption so that no verification errors -- REMOVE LATER
      assert(!err) // This is also maybe not we want (Viper will really try to check this)

      // --- END OF BLOCK 2 ------------------------------------------------------- //
      // ^-- This part requires some revision --------------------------------------//

      /* var stat = TXD_READY | TXD_ADDCRC | TXD_LAST; // uint16_t
         hw_ring_get_tail(hw_tail, hw_ring_tx)
         if ((hw_tail + 1) == TX_COUNT) {
               stat = (stat | WRAP) & MASK_16;
         }
         hw_ring_get_descr_mdata(buff, hw_tail, hw_ring_tx)
         var buffer = lds {2} buffer_addr;
         !stw buff, buffer.0;
         !stw buff + WORD_SIZE, buffer.1;
      */

      var stat: myBV := or(TXD_READY, or(TXD_ADDCRC, TXD_LAST)) // Could make this a 16-len bitvector
      // Expanded macro for `hw_ring_get_tail(hw_tail, hw_ring_tx)`
      var hw_tail: myBV := toBV64(0)
      shared_ldw(hw_tail, hw_ring_tx)

      if (add(hw_tail,toBV64(1)) == TX_COUNT) {
         stat := and(or(stat, WRAP), MASK_16)
      }
      // Expanded macro for `hw_ring_get_descr_mdata(buff, hw_tail, hw_ring_tx)`
      var buff: myBV := add(hw_ring_tx,add(toBV64(16),mul(hw_tail, NET_BUFF_DESC_SIZE)))

      var buffer: Record := lds(cons_shape(Seq(2)), buffer_addr)
      shared_stw(buff, get_word_field(buffer,0))
      shared_stw(add(buff, WORD_SIZE), get_word_field(buffer,1))
      
      // --- END OF BLOCK 3 ------------------------------------------------------- //
    
      /* 
         get_io_or_offset(io_or_offset, buffer_addr)
         get_len(len, buffer_addr)
         update_ring_slot(hw_ring_tx, hw_tail, io_or_offset, len, stat)
      */

      // Expanded macro for `get_io_or_offset(io_or_offset, buffer_addr)`
      var io_or_offset: myBV := toBV64(0)
      shared_ldw(io_or_offset, buffer_addr)
      // Expanded macro for `get_len(len, buffer_addr)`
      var len: myBV := toBV64(0)
      shared_ldw(len, add(buffer_addr,WORD_SIZE))
      // Expanded macro for `update_ring_slot(hw_ring_tx, hw_tail, io_or_offset, len, stat)`
         // Expanded macro for `hw_ring_get_descr(descr, hw_ring_tx)
      var addr: myBV := add(hw_ring_tx,add(mul(toBV64(2),WORD_SIZE),mul(MAX_COUNT,NET_BUFF_DESC_SIZE))) 
      var descr_R: Record := lds(shape_One(), addr)
      // Next line is because of how I've set up shapes..
      var descr: myBV := get_word_field(descr_R,0) 

      var dst_addr: myBV := add(descr, mul(hw_tail,DESCRIPTOR_SIZE))
      var descriptor: myBV := or(shl(stat, toBV64(16)), len)
      shared_st32(dst_addr, descriptor, HW_state)
      shared_st32(add(dst_addr,toBV64(4)), io_or_offset, HW_state)

      // --- END OF BLOCK 4 ------------------------------------------------------- //
  
      /* 
         hw_tail = hw_tail + 1;
         pnk_modulo(mod, hw_tail, TX_COUNT)
         hw_ring_set_tail(mod, hw_ring_tx)
      */

      hw_tail := add(hw_tail, toBV64(1))
      // Expanded macro for `pnk_modulo(mod, hw_tail, TX_COUNT)`
      var pnk_modulo_mask_1: myBV := sub(TX_COUNT, toBV64(1))
      var mod_1: myBV := and(hw_tail, pnk_modulo_mask_1)

      // Expanded macro for `hw_ring_set_tail(mod, hw_ring_tx)`
      shared_stw(hw_ring_tx, mod_1)

      // --- END OF BLOCK 5 ------------------------------------------------------- //

      /*
         get_eth_tdar(tdar, eth)
         if (!(tdar & TDAR_TDAR)) {
            set_eth_tdar(TDAR_TDAR, eth)
         }
      */
      // Expanded macro for `get_eth_tdar(tdar, eth)`
      var tdar: myBV := toBV64(0)
      shared_ld32(tdar, add(eth, TDAR_OFFSET), HW_state)

      if (!toBool(and(tdar, TDAR_TDAR))) {
         // Expanded macro for `set_eth_tdar(TDAR_TDAR, eth)
         shared_st32(add(eth, TDAR_OFFSET), tdar, HW_state)
      }

    }
   /*
    net_request_signal_active(tx_queue)
    reprocess = false;
   */
   // Expanded macro for `net_request_signal_active(tx_queue)`
      // Expanded macro for `get_active_queue(queue, tx_queue)
   var queue: myBV := toBV64(0)
   shared_ldw(queue, add(tx_queue, WORD_SIZE))
      // Expanded macro for `clear_consumer_signalled(queue)`
   var signal: myBV := toBV64(0)
   shared_st8(add(queue,mul(WORD_SIZE, toBV64(2))),signal) // ??

   EXT_THREAD_MEMORY_RELEASE(bv_zero,bv_zero,bv_zero,bv_zero)
   reprocess := false

   /*
      hw_ring_full(full, hw_ring_tx, TX_COUNT)
      net_queue_empty_active(empty, tx_queue)
      if ((!full) && (!empty)) {
         net_cancel_signal_active(tx_queue)
         reprocess = true;
      } 
   */

   // Expanded macro for `hw_ring_full(full, hw_ring_tx, TX_COUNT)
      // Expanded macro for `hw_ring_get_tail(tail, hw_ring_tx)` 
      var tail_1: myBV := toBV64(0)
      shared_ldw(tail_1, hw_ring_tx)

      // Expanded macro for `hw_ring_get_head(head, hw_ring_tx)`
      var head_1: myBV := toBV64(0)
      shared_ldw(head_1, add(hw_ring_tx, WORD_SIZE))

      var value_1 : myBV := sub(tail_1, add(head_1, toBV64(1)))
      // Expanded macro for `pnk_modulo(mod, value, TX_COUNT)`
      var pnk_modulo_mask_2: myBV := sub(TX_COUNT, toBV64(1))
      var mod_2: myBV := and(value_1, pnk_modulo_mask_2)

      var full_1: Bool := (toInt(mod_2) == 0)

   // Expanded macro for `net_queue_empty_active(empty, tx_queue)`
     //  Expanded macro for `get_active_tail(tail, tx_queue)`                 
         // Expanded macro for `get_active_queue(tail, tx_queue)`
         tail_1 := toBV64(0)
         shared_ldw(tail_1, add(tx_queue, WORD_SIZE))

         // Expanded macro for `get_tail(tail, tx_queue)`
         tail_1 := toBV64(0)
         shared_ldw(tail_1, tx_queue)

     //  Expanded macro for `get_active_head(head, tx_queue)`                 
         // Expanded macro for `get_active_queue(head, tx_queue)`
         tail_1 := toBV64(0)
         shared_ldw(tail_1, add(tx_queue, WORD_SIZE))

         // `get_head(head, tx_queue)`
         head_1 := toBV64(0)
         shared_ldw(head_1, add(tx_queue, WORD_SIZE))

   var empty_1: Bool := toInt(sub(tail_1,head_1)) == 0                     

   if (!full_1 && !empty_1) {
      // Expanded macro for `net_cancel_signal_active(tx_queue)
        // Expanded macro for `get_free_queue(queue, tx_queue)`
      queue := toBV64(0)
      shared_ldw(queue, tx_queue)
        // Expanded macro for `set_consumer_signalled(queue)`
      signal := toBV64(1)
      shared_st8(add(queue,mul(toBV64(2),WORD_SIZE)), signal)
      EXT_THREAD_MEMORY_RELEASE(bv_zero, bv_zero, bv_zero, bv_zero)

      reprocess := true
   }
  }
  r := 0
}

domain Event {
   // Enumerate possible kinds of device events
   // These are just examples, I'm not sure what we would need here 
   function read_event(): Event
   function write_event(): Event 

   // Listing these out could get annoying if many kinds of events
   axiom events_distinct {
      read_event() != write_event()
   } 
}
// or maybe it would suffice to just have the trace of past states 

/* Device Model Ideas */

// device state is a Ref object with fields

// OLD IMPLEMENTATION
/*
field log: Seq[Event]
field curr_state: Int

method device_transition(a: myBV, acc_type: Bool, data: myBV, device_state: Ref)
   requires true // Pass permissions to memory-mapped device registers?
   ensures true  // and then return (?) these permissions afterwards
   // If these permissions vary significantly, might break this device transition into
   //    multiple different methods
{
   inhale acc(device_state.log)
   inhale acc(device_state.curr_state)

   // Insert transition fn details here.
   // Should transition to next state based on `device_state.curr_state` & given inputs
   //   & update any device registers, add to `device_state.log` accordingly
}
*/

// Meeting comments
// Modelling of header flags for each packet? Note `wrap` flag significantly chages behaviour
//   doing this properly may require an array of flags 

// Part of virt implementation: 
//   - assume that we never exceed the max number of buffers, since virtualiser 
//       will only use free buffers to pass info, and these buffers are provided
//       by the device itself

// Start of larger model which includes OS-side interactions
field VRT_n_empty_bufs: Int
field VRT_n_active_bufs: Int

define total_n_bufs 8
// Any of the following might suffice to show that no active buffers are overwritten before
//   they are processed
// ensure st.HWS_n_active_bufs + st.VRT_n_empty_bufs + st.VRT_active_bufs == total_n_bufs

define VRT_acc(st) acc(st.VRT_n_active_bufs) && acc(st.VRT_n_empty_bufs)

method dev_init(HWS: Ref) 
   requires HWS_acc(HWS)
   // acc(HWS.HWS_n_active_bufs) && acc(HWS.HWS_head) && acc(HWS.HWS_tail) 
   ensures HWS_acc(HWS)
   // acc(HWS.HWS_n_active_bufs) && acc(HWS.HWS_head) && acc(HWS.HWS_tail) 
   ensures HWS.HWS_head == 0 && HWS.HWS_n_active_bufs == 0 && HWS.HWS_tail == 0 

function ND_select(n_active_bufs : Int): Int 
   ensures 0 <= result && result <= n_active_bufs

// RX-side HW model
method device_transition(HWS: Ref) 
   requires acc(HWS.HWS_n_active_bufs) && acc(HWS.HWS_head) && acc(HWS.HWS_tail) 
   ensures true // Should look something like example from paper 
{ 
   
   // Implement transition which respects postcondition

   if (HWS.HWS_n_active_bufs == 0) {}
   else { 
      var x: Int := ND_select(HWS.HWS_n_active_bufs)
      HWS.HWS_head := HWS.HWS_head + x  // modulo 8? Might complicate reasoning
      HWS.HWS_n_active_bufs := HWS.HWS_head - x
   }

}

// Device HW ring client-side accesses now depend on state
// These model query functions should be used within shared_stw--, etc.

// Writing a buffer, then setting the header to notify device
//   --> increment the number of active buffers and tail by 1

// Every time we check whether the i-th buffer is available
//   --> trigger a ND transition
//   --> depending on results, decide if buffer free or not

// How to easily identify the index of the current buffer from a given shared memory 
//   access ?? 
// Convert `tail`, `head` to ints? Is BV <-> Int casting avoidable somehow?



